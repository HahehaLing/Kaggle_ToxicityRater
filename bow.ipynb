{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KnUPQ03ozgN6"
   },
   "source": [
    "# Comment Toxicity Severity Rater\n",
    "- group by the same comment and find out if that comment is more often rated as more/ less toxic \n",
    "- transform the input data to be more / less toxic 0 and 1 // train on these\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IR61GQ1Qywzb",
    "outputId": "67ef639b-e210-4526-ff0f-b484165995f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: pip\n",
      "zsh:1: command not found: pip\n",
      "zsh:1: command not found: pip\n"
     ]
    }
   ],
   "source": [
    "#!pip install alt-profanity-check\n",
    "!pip install joblib\n",
    "!pip install scikit-learn\n",
    "#install tweet-preprocessor to clean tweets\n",
    "!pip install tweet-preprocessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "JK2ClqlvzPs7"
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "YBi80m2m0wO4"
   },
   "outputs": [],
   "source": [
    "# remove special characters using the regular expression library\n",
    "# updated with . and = \n",
    "import re\n",
    "\n",
    "#set up punctuations we want to be replaced\n",
    "REPLACE_NO_SPACE = re.compile(\"(\\*)|(\\=\\=)|(\\~) | (\\=) | (\\.\\.\\.) |(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\|)|(\\()|(\\))|(\\[)|(\\])|(\\%)|(\\$)|(\\>)|(\\<)|(\\{)|(\\})\")\n",
    "REPLACE_WITH_SPACE = re.compile(\" (<br\\s/><br\\s/?)| (\\n\\n) | (\\n) |(\\.) |(-)|(/)|(:). \")\n",
    "\n",
    "#re.escape (* )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# re.sub() // natalie uses this more often\n",
    "\n",
    "# regex has a method to escape the char properly \n",
    "re.escape\n",
    "\n",
    "\n",
    "# gridsearchcv() // can be slow \n",
    "if have a few val, can use \n",
    "\n",
    "# use randomsearch() // possible range // faster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "NsFs8bzc0ylC"
   },
   "outputs": [],
   "source": [
    "import preprocessor as p\n",
    "\n",
    "# custum function to clean the dataset (combining tweet_preprocessor and reguar expression)\n",
    "def clean_tweets(df):\n",
    "  tempArr = []\n",
    "  for line in df:\n",
    "    # send to tweet_processor\n",
    "    tmpL = p.clean(line)\n",
    "    # remove puctuation\n",
    "    tmpL = REPLACE_NO_SPACE.sub(\"\", tmpL.lower()) # convert all tweets to lower cases\n",
    "    tmpL = REPLACE_WITH_SPACE.sub(\" \", tmpL)\n",
    "    tempArr.append(tmpL)\n",
    "  return tempArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jdxdQ2Fz00L3"
   },
   "outputs": [],
   "source": [
    "original_data = pd.read_csv(\"./data/validation_data.csv\")\n",
    "\n",
    "data_to_test = pd.read_csv('./data/comments_to_score.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Jcgva1U5yJYN",
    "outputId": "478c0002-a327-4233-a62c-c62d16907adf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>worker</th>\n",
       "      <th>less_toxic</th>\n",
       "      <th>more_toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>313</td>\n",
       "      <td>This article sucks \\n\\nwoo woo wooooooo</td>\n",
       "      <td>WHAT!!!!!!!!?!?!!?!?!!?!?!?!?!!!!!!!!!!!!!!!!!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>188</td>\n",
       "      <td>\"And yes, people should recognize that but the...</td>\n",
       "      <td>Daphne Guinness \\n\\nTop of the mornin' my fav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>82</td>\n",
       "      <td>Western Media?\\n\\nYup, because every crime in...</td>\n",
       "      <td>\"Atom you don't believe actual photos of mastu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>347</td>\n",
       "      <td>And you removed it! You numbskull! I don't car...</td>\n",
       "      <td>You seem to have sand in your vagina.\\n\\nMight...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>539</td>\n",
       "      <td>smelly vagina \\n\\nBluerasberry why don't you ...</td>\n",
       "      <td>hey \\n\\nway to support nazis, you racist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   worker                                         less_toxic  \\\n",
       "0     313            This article sucks \\n\\nwoo woo wooooooo   \n",
       "1     188  \"And yes, people should recognize that but the...   \n",
       "2      82   Western Media?\\n\\nYup, because every crime in...   \n",
       "3     347  And you removed it! You numbskull! I don't car...   \n",
       "4     539   smelly vagina \\n\\nBluerasberry why don't you ...   \n",
       "\n",
       "                                          more_toxic  \n",
       "0  WHAT!!!!!!!!?!?!!?!?!!?!?!?!?!!!!!!!!!!!!!!!!!...  \n",
       "1   Daphne Guinness \\n\\nTop of the mornin' my fav...  \n",
       "2  \"Atom you don't believe actual photos of mastu...  \n",
       "3  You seem to have sand in your vagina.\\n\\nMight...  \n",
       "4           hey \\n\\nway to support nazis, you racist  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "original_data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "5beXEglKyJYO",
    "outputId": "212d55c9-b984-47ed-dd95-8538a49ce5d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WHAT!!!!!!!!?!?!!?!?!!?!?!?!?!!!!!!!!!!!!!!!!!!!!!!!!??????????????????????????????????????????????????????!!!!!!=\\nWHER IS YOUR SEXY PIC GONE FROM YOUR MAIN PAGE???????? put it back'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#original_data.iloc[0]\n",
    "original_data['more_toxic'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "4Ned_u-3yJYO",
    "outputId": "d01310d0-1d89-4cb0-d6f4-41fb292e9213"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>worker</th>\n",
       "      <th>less_toxic</th>\n",
       "      <th>more_toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>313</td>\n",
       "      <td>This article sucks \\n\\nwoo woo wooooooo</td>\n",
       "      <td>WHAT!!!!!!!!?!?!!?!?!!?!?!?!?!!!!!!!!!!!!!!!!!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>188</td>\n",
       "      <td>\"And yes, people should recognize that but the...</td>\n",
       "      <td>Daphne Guinness \\n\\nTop of the mornin' my fav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>82</td>\n",
       "      <td>Western Media?\\n\\nYup, because every crime in...</td>\n",
       "      <td>\"Atom you don't believe actual photos of mastu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>347</td>\n",
       "      <td>And you removed it! You numbskull! I don't car...</td>\n",
       "      <td>You seem to have sand in your vagina.\\n\\nMight...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>539</td>\n",
       "      <td>smelly vagina \\n\\nBluerasberry why don't you ...</td>\n",
       "      <td>hey \\n\\nway to support nazis, you racist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30103</th>\n",
       "      <td>461</td>\n",
       "      <td>I'm sorry. I'm not an admin. I will give you t...</td>\n",
       "      <td>get out my large penis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30104</th>\n",
       "      <td>527</td>\n",
       "      <td>I'm sorry. I'm not an admin. I will give you t...</td>\n",
       "      <td>get out my large penis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30105</th>\n",
       "      <td>352</td>\n",
       "      <td>wow...\\nare you out of your mind, how was my e...</td>\n",
       "      <td>Piss off you slant eyed-gook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30106</th>\n",
       "      <td>311</td>\n",
       "      <td>wow...\\nare you out of your mind, how was my e...</td>\n",
       "      <td>Piss off you slant eyed-gook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30107</th>\n",
       "      <td>54</td>\n",
       "      <td>wow...\\nare you out of your mind, how was my e...</td>\n",
       "      <td>Piss off you slant eyed-gook</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30108 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       worker                                         less_toxic  \\\n",
       "0         313            This article sucks \\n\\nwoo woo wooooooo   \n",
       "1         188  \"And yes, people should recognize that but the...   \n",
       "2          82   Western Media?\\n\\nYup, because every crime in...   \n",
       "3         347  And you removed it! You numbskull! I don't car...   \n",
       "4         539   smelly vagina \\n\\nBluerasberry why don't you ...   \n",
       "...       ...                                                ...   \n",
       "30103     461  I'm sorry. I'm not an admin. I will give you t...   \n",
       "30104     527  I'm sorry. I'm not an admin. I will give you t...   \n",
       "30105     352  wow...\\nare you out of your mind, how was my e...   \n",
       "30106     311  wow...\\nare you out of your mind, how was my e...   \n",
       "30107      54  wow...\\nare you out of your mind, how was my e...   \n",
       "\n",
       "                                              more_toxic  \n",
       "0      WHAT!!!!!!!!?!?!!?!?!!?!?!?!?!!!!!!!!!!!!!!!!!...  \n",
       "1       Daphne Guinness \\n\\nTop of the mornin' my fav...  \n",
       "2      \"Atom you don't believe actual photos of mastu...  \n",
       "3      You seem to have sand in your vagina.\\n\\nMight...  \n",
       "4               hey \\n\\nway to support nazis, you racist  \n",
       "...                                                  ...  \n",
       "30103                             get out my large penis  \n",
       "30104                             get out my large penis  \n",
       "30105                       Piss off you slant eyed-gook  \n",
       "30106                       Piss off you slant eyed-gook  \n",
       "30107                       Piss off you slant eyed-gook  \n",
       "\n",
       "[30108 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data = original_data\n",
    "cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "3ynSbwXpyJYP"
   },
   "outputs": [],
   "source": [
    "clean_less_toxic = clean_tweets(original_data[\"less_toxic\"])\n",
    "cleaned_data['less_toxic'] = pd.DataFrame(clean_less_toxic)\n",
    "#cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "JUnN_J62yJYP",
    "outputId": "cc219ae9-801f-4413-e66b-179801803916"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>worker</th>\n",
       "      <th>less_toxic</th>\n",
       "      <th>more_toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>313</td>\n",
       "      <td>this article sucks woo woo wooooooo</td>\n",
       "      <td>what=wher is your sexy pic gone from your main...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>188</td>\n",
       "      <td>and yes people should recognize that but they ...</td>\n",
       "      <td>daphne guinness top of the mornin my favourite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>82</td>\n",
       "      <td>western mediayup because every crime in the en...</td>\n",
       "      <td>atom you dont believe actual photos of masturb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>347</td>\n",
       "      <td>and you removed it you numbskull i dont care w...</td>\n",
       "      <td>you seem to have sand in your vagina.might wan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>539</td>\n",
       "      <td>smelly vagina bluerasberry why dont you be a m...</td>\n",
       "      <td>hey way to support nazis you racist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30103</th>\n",
       "      <td>461</td>\n",
       "      <td>im sorry im not an admin i will give you three...</td>\n",
       "      <td>get out my large penis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30104</th>\n",
       "      <td>527</td>\n",
       "      <td>im sorry im not an admin i will give you three...</td>\n",
       "      <td>get out my large penis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30105</th>\n",
       "      <td>352</td>\n",
       "      <td>wow...are you out of your mind how was my edit...</td>\n",
       "      <td>piss off you slant eyed gook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30106</th>\n",
       "      <td>311</td>\n",
       "      <td>wow...are you out of your mind how was my edit...</td>\n",
       "      <td>piss off you slant eyed gook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30107</th>\n",
       "      <td>54</td>\n",
       "      <td>wow...are you out of your mind how was my edit...</td>\n",
       "      <td>piss off you slant eyed gook</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30108 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       worker                                         less_toxic  \\\n",
       "0         313                this article sucks woo woo wooooooo   \n",
       "1         188  and yes people should recognize that but they ...   \n",
       "2          82  western mediayup because every crime in the en...   \n",
       "3         347  and you removed it you numbskull i dont care w...   \n",
       "4         539  smelly vagina bluerasberry why dont you be a m...   \n",
       "...       ...                                                ...   \n",
       "30103     461  im sorry im not an admin i will give you three...   \n",
       "30104     527  im sorry im not an admin i will give you three...   \n",
       "30105     352  wow...are you out of your mind how was my edit...   \n",
       "30106     311  wow...are you out of your mind how was my edit...   \n",
       "30107      54  wow...are you out of your mind how was my edit...   \n",
       "\n",
       "                                              more_toxic  \n",
       "0      what=wher is your sexy pic gone from your main...  \n",
       "1      daphne guinness top of the mornin my favourite...  \n",
       "2      atom you dont believe actual photos of masturb...  \n",
       "3      you seem to have sand in your vagina.might wan...  \n",
       "4                    hey way to support nazis you racist  \n",
       "...                                                  ...  \n",
       "30103                             get out my large penis  \n",
       "30104                             get out my large penis  \n",
       "30105                       piss off you slant eyed gook  \n",
       "30106                       piss off you slant eyed gook  \n",
       "30107                       piss off you slant eyed gook  \n",
       "\n",
       "[30108 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_more_toxic = clean_tweets(original_data[\"more_toxic\"])\n",
    "cleaned_data['more_toxic'] = pd.DataFrame(clean_more_toxic)\n",
    "cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mCLOOh9myJYP",
    "outputId": "b3e6849d-429b-4806-99d7-12a876701294"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cody porter dies while foolishly copying kimimaro kaguya    okay we need to add this seriously i mean we added the crap about a freakin death note so i believe we definitely should add this especially since a little ten year old baka died     news report t  c     this has already been discussed before here check that discussion for the news reports and why we decided not to include it also he was imitating gaara not kimmimaro warranting the nickname of s& sand hero from  b  got a present for ya  mission log   heres where it went wrong  no he was copying kimimaro dont believe every little detail from a reporter who calls it nerutu sand ninjas.. gaara was using armor he wasnt actually buried sorry if i sound aggressive but i know my naruto   i seriosly dont want to be rude but these parents and a whole ton of other people are clueless they all think that naruto is some weird kids show like doraemon or pokmon no it was made for teenagers well in the of age or older and was serialized in a teenager magazine its stupid these companys make all of em look like pokmon so they all think they are for little children now look at what you got a clueless news reporter calling it neruto sand ninjas and a bunch of perverted little children saying haha sexy justu haha it really gets on my nerves  the free encyclopedia   nah it was gaara teh internets have decided that he was imitating gaara dubbing him the sand hero and i have no reason to doubt the accuracy of the internets got a present for ya  mission log     if he were copying kimimaro hed be trying to remove his femur for use in a rousing game of hoop and stick now that would be worth watching ~to     i dont know whether to find that hilarious or disturbing perhaps both t  c  oh by the way leafninja isnt a fansite.\n",
      "11503\n",
      "11644\n"
     ]
    }
   ],
   "source": [
    "grouped_df_less = original_data.groupby(['less_toxic']).size()\n",
    "\n",
    "print(grouped_df_less.index[0])\n",
    "#grouped_df_less.values[0]\n",
    "\n",
    "grouped_df_more = original_data.groupby(['more_toxic']).size()\n",
    "#grouped_df_more.index[0]\n",
    "\n",
    "print(len(grouped_df_less.values))\n",
    "print(len(grouped_df_more.values))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zazhbJmByJYQ"
   },
   "source": [
    "# Transform the original data to be grouped by the comment and rate as more toxic or less toxic based on the more often rated comment\n",
    "\n",
    "# todo: implement jason's sort indices method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ldG9oY4gyJYQ"
   },
   "outputs": [],
   "source": [
    "n_less = len(grouped_df_less.index)\n",
    "n_more = len(grouped_df_more.index)\n",
    "\n",
    "comment = []\n",
    "score = []\n",
    "#jason jan 29: sort the indices (think of the comments as numbers, == and > and <, if a < b, then up the a), if the indices are the same - do logic,when more index is less than the less index // inc whatever is lesser than the other index. Once the threshold to the \"more valued\" index, then append the rest into the new transformed \n",
    "for i in range(n_more):\n",
    "    if grouped_df_more.index[i] not in grouped_df_less.index:\n",
    "            comment.append(grouped_df_more.index[i])\n",
    "            score.append(1)\n",
    "            continue\n",
    "    for j in range(n_less): \n",
    "        \n",
    "        if grouped_df_less.index[j] == grouped_df_more.index[i]:\n",
    "            comment.append(grouped_df_less.index[j])\n",
    "            if grouped_df_more.values[i] >= grouped_df_less.values[j]:\n",
    "                score.append(1)\n",
    "            else:\n",
    "                score.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cdFVrA97yJYQ",
    "outputId": "88df7da1-d5ea-417a-cef3-6a9c99932bf6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11644\n",
      "11644\n"
     ]
    }
   ],
   "source": [
    "print(len(comment))\n",
    "print(len(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "c-hwm4IyyJYQ",
    "outputId": "3706cc9d-9396-4533-8722-e654fa02524d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>=            =                    ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cody porter dies while foolishly copying ki...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i get it the page is about all the doomsday...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>invention   what on earth are you talking a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is she black   why dose it matter what race...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11639</th>\n",
       "      <td>zora you seem to assume that i have not done m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11640</th>\n",
       "      <td>zulfiqar ali bhutto stop vandalizing zulfiqar ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11641</th>\n",
       "      <td>zzzzzzz.. youre a real bore now go bore someon...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11642</th>\n",
       "      <td>~whats with you that youre so annoying and pro...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11643</th>\n",
       "      <td>~~its okay of course for them to have their ow...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11644 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 comment  score\n",
       "0                  =            =                    ...      1\n",
       "1         cody porter dies while foolishly copying ki...      0\n",
       "2         i get it the page is about all the doomsday...      0\n",
       "3         invention   what on earth are you talking a...      0\n",
       "4         is she black   why dose it matter what race...      1\n",
       "...                                                  ...    ...\n",
       "11639  zora you seem to assume that i have not done m...      1\n",
       "11640  zulfiqar ali bhutto stop vandalizing zulfiqar ...      1\n",
       "11641  zzzzzzz.. youre a real bore now go bore someon...      0\n",
       "11642  ~whats with you that youre so annoying and pro...      1\n",
       "11643  ~~its okay of course for them to have their ow...      1\n",
       "\n",
       "[11644 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans = pd.DataFrame(comment, columns = ['comment'])\n",
    "trans['score'] = pd.DataFrame(score, columns = ['score'])\n",
    "trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "AJpTleHdyJYQ"
   },
   "outputs": [],
   "source": [
    "# these comments are all categorized as less toxic \n",
    "for j in range(n_less): \n",
    "        if grouped_df_less.index[j] not in grouped_df_more.index:\n",
    "            comment.append(grouped_df_less.index[j])\n",
    "            score.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dv1Wlg7WyJYR",
    "outputId": "e152ff1a-02f0-415d-b9d9-399d0f74846a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14208\n",
      "14208\n"
     ]
    }
   ],
   "source": [
    "print(len(comment))\n",
    "print(len(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "U4lf_BoSyJYR"
   },
   "outputs": [],
   "source": [
    "trans = pd.DataFrame(comment, columns = ['comment'])\n",
    "trans['score'] = pd.DataFrame(score, columns = ['score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "uboSHkfpyJYR",
    "outputId": "ac1041c5-b05a-4262-d6f8-9f4777bf025d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>14208.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0\n",
       "count  14208.0\n",
       "mean       1.0\n",
       "std        0.0\n",
       "min        1.0\n",
       "25%        1.0\n",
       "50%        1.0\n",
       "75%        1.0\n",
       "max        1.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing to see if the transformation data only appears once now (expected). Want every comment\n",
    "# to be either more or less toxic\n",
    "pd.DataFrame(trans.groupby(['comment']).size().values).describe() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BjoQQTZIyJYR",
    "outputId": "d76171e3-66a9-440d-8043-cb5f4a47c60d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "score\n",
       "0    6765\n",
       "1    7443\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans.groupby(['score']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "wwRA4cZQyJYR",
    "outputId": "5315474f-3c30-483f-85c6-3886a3445cdd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>=            =                    ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cody porter dies while foolishly copying ki...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i get it the page is about all the doomsday...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>invention   what on earth are you talking a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>is she black   why dose it matter what race...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14203</th>\n",
       "      <td>yuber youre the pov pusher and stalking my con...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14204</th>\n",
       "      <td>yuberplease do something about yubers constant...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14205</th>\n",
       "      <td>ywhy arnt i allowed to contribute when i just ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14206</th>\n",
       "      <td>zionist jewplease stop spreadin your lies bias...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14207</th>\n",
       "      <td>~i heard that dick gephardt was actually presi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14208 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 comment  score\n",
       "0                  =            =                    ...      1\n",
       "1         cody porter dies while foolishly copying ki...      0\n",
       "2         i get it the page is about all the doomsday...      0\n",
       "3         invention   what on earth are you talking a...      0\n",
       "4         is she black   why dose it matter what race...      1\n",
       "...                                                  ...    ...\n",
       "14203  yuber youre the pov pusher and stalking my con...      0\n",
       "14204  yuberplease do something about yubers constant...      0\n",
       "14205  ywhy arnt i allowed to contribute when i just ...      0\n",
       "14206  zionist jewplease stop spreadin your lies bias...      0\n",
       "14207  ~i heard that dick gephardt was actually presi...      0\n",
       "\n",
       "[14208 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "c83rXllE12qD"
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# extract the labels from the train data\n",
    "y = trans.score.values\n",
    "X = trans.comment.values\n",
    "\n",
    "# use 60% for the training, 20% validation, and 20% for the test\n",
    "x_train_all, x_test, y_train_all, y_test = train_test_split(X, y, \n",
    "                                                    stratify=y, \n",
    "                                                    random_state=1, \n",
    "                                                    test_size=0.2, shuffle=True)\n",
    "# train\n",
    "x_train = x_train_all[int(x_train_all.shape[0]/4):]\n",
    "y_train = y_train_all[int(y_train_all.shape[0]/4):]\n",
    "\n",
    "#valid\n",
    "x_val = x_train_all[:int(x_train_all.shape[0]/4)]\n",
    "y_val = y_train_all[:int(y_train_all.shape[0]/4)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wZS2wKuIJulw",
    "outputId": "28eac730-b873-46be-bf4b-7af8cf189439"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "T4rexB1zyJYS",
    "outputId": "ea696975-fc17-47a4-9edf-9b8f7522c710"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>114890</td>\n",
       "      <td>gjalexei you asked about whether there is an ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>732895</td>\n",
       "      <td>looks like be have an abuser  can you please l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1139051</td>\n",
       "      <td>i confess to having complete and apparently bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1434512</td>\n",
       "      <td>freuds ideas are certainly much discussed toda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2084821</td>\n",
       "      <td>it is not just you this is a laundry list of s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7532</th>\n",
       "      <td>504235362</td>\n",
       "      <td>go away you annoying vandal.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7533</th>\n",
       "      <td>504235566</td>\n",
       "      <td>this user is a vandal.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7534</th>\n",
       "      <td>504308177</td>\n",
       "      <td>sorry to sound like a pain but one by followi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7535</th>\n",
       "      <td>504570375</td>\n",
       "      <td>well its pretty fucking irrelevant now im unbl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7536</th>\n",
       "      <td>504598250</td>\n",
       "      <td>the team name is great britain and northern ir...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7537 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      comment_id                                               text\n",
       "0         114890   gjalexei you asked about whether there is an ...\n",
       "1         732895  looks like be have an abuser  can you please l...\n",
       "2        1139051  i confess to having complete and apparently bl...\n",
       "3        1434512  freuds ideas are certainly much discussed toda...\n",
       "4        2084821  it is not just you this is a laundry list of s...\n",
       "...          ...                                                ...\n",
       "7532   504235362                       go away you annoying vandal.\n",
       "7533   504235566                             this user is a vandal.\n",
       "7534   504308177   sorry to sound like a pain but one by followi...\n",
       "7535   504570375  well its pretty fucking irrelevant now im unbl...\n",
       "7536   504598250  the team name is great britain and northern ir...\n",
       "\n",
       "[7537 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clean the input for predition data\n",
    "cleaned_text = clean_tweets(data_to_test['text'])\n",
    "\n",
    "data_to_test['text'] = pd.DataFrame(cleaned_text)\n",
    "data_to_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "uVP8iy3t6qtG"
   },
   "outputs": [],
   "source": [
    "# extract the labels from the train data\n",
    "#y = trans.score.values\n",
    "#X = trans.comment.values\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# initalize vectorizer: vectorize tweets for model building\n",
    "vectorizer = CountVectorizer(binary=True, stop_words='english') #stop_words removed: a, the\n",
    "\n",
    "# learn a vocabulary dictionary of all tokens in the raw documents\n",
    "vectorizer.fit(list(x_train_all) + list(x_test) + list(data_to_test[\"text\"].values))\n",
    "    \n",
    "\n",
    "# transform documents to document-term matrix\n",
    "x_train_vec = vectorizer.transform(x_train)\n",
    "x_train_val_vec = vectorizer.transform(x_train_all)\n",
    "x_val_vec = vectorizer.transform(x_val)\n",
    "\n",
    "\n",
    "x_test_vec = vectorizer.transform(x_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vsL42CiA6aS5",
    "outputId": "fdabc591-86d9-4bd5-fa11-43f674575cab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_val_vec.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B5UlbZEw6rB-",
    "outputId": "74652e54-4bb7-4848-d068-04f2beec84ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11366, 46974)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train_val_vec.toarray().shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CjSGWkpR4p5A",
    "outputId": "57cc3e6b-a5f3-4c7c-b8d8-d200b814bc24"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46974"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train_val_vec.toarray()[0].size) #number of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "RsLQG2jwgibE"
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# initalize vectorizer: vectorize tweets for model building\n",
    "vectorizer = CountVectorizer(binary=True, stop_words='english') #stop_words removed: a, the\n",
    "\n",
    "# learn a vocabulary dictionary of all tokens in the raw documents\n",
    "vectorizer.fit(list(cleaned_data['less_toxic'].values) + list(cleaned_data['more_toxic'].values))\n",
    "    \n",
    "\n",
    "# transform documents to document-term matrix\n",
    "less_tox_vec = vectorizer.transform(cleaned_data['less_toxic'])\n",
    "more_tox_vec = vectorizer.transform(cleaned_data['more_toxic'])\n",
    "\n",
    "\n",
    "#concatenate on the last dim\n",
    "# same rows, vec from first one on the sec vec\n",
    "\n",
    "# zip and train  on the dataframe -- 1 column for more toxic, 1 for less toxic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# concatenate on the last dim\n",
    "# same rows, vec from first one on the sec vec\n",
    "\n",
    "# TODO: LOOK AT SLACK WITH NATALIE FOR NORMAL MODEL TRAIN 2) LISTEN TO THE PHONE RECORDING FOR LSTM AND LOGISTIC REGRESSION TRAIN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<30108x46974 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 800699 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "less_tox_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 features ? \n",
    "\n",
    "for part 1 and part 2? \n",
    "\n",
    "## concatenate the whole vector count\n",
    "\n",
    "# but how to tell the model that feature 1 of column 1 is for lesser -- how to put a label on that ? when the same row has another column for more toxic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fiI0nmfv05md"
   },
   "source": [
    "# comparing the classification accuracy of basic models // should place after the cross-validated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "ZKNXtETtcb81"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import rankdata\n",
    "# make a function to score the prediction and compare if the \"lesser\" toxic rated is lower prob of rated toxicity\n",
    "def comp_toxicity (model, less_tox_val_vec, more_tox_val_vec):\n",
    "  '''\n",
    "   model is the trained model to be evaluated against the less and more toxic comments\n",
    "    objective is to see if more toxic comments have a higher score\n",
    "  \n",
    "  less_tox_val_vec and more_tox_val_vec are the respective column in dataframe that contains the comments vectorized\n",
    "  '''\n",
    "  result = res = np.zeros((1,2))\n",
    "  for i in range(cleaned_data.shape[0]):\n",
    "    result = rankdata([model.predict_proba(less_tox_val_vec[i]).tolist()[0][1], \n",
    "             model.predict_proba(more_tox_val_vec[i]).tolist()[0][1]], method = 'ordinal' )\n",
    "    result -= 1\n",
    "    #print(result)\n",
    "    res += result\n",
    "    #print(res[0])\n",
    "    \n",
    "    \n",
    "  print ('final result: ',res[0])\n",
    "  return res[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bxFOh2JRkELg",
    "outputId": "758aff87-be81-4a6d-c52e-7da477e1eedc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=10, random_state=1)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=10, random_state=1)\n",
    "clf.fit(x_train_vec,y_train )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w9vJ8s2gwE-T",
    "outputId": "238fff8c-0339-4c05-de40-8abcb010e8b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final result:  [ 7179. 22929.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 7179., 22929.])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_toxicity(clf, less_tox_vec, more_tox_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U8KtIcgUrLd5",
    "outputId": "5979915e-721e-4552-8ce4-f77f71a265a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final result:  [ 5961. 24147.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 5961., 24147.])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "clf = neighbors.KNeighborsClassifier(n_neighbors=3)\n",
    "clf.fit(x_train_vec,y_train )\n",
    "comp_toxicity(clf, less_tox_vec, more_tox_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8CYNXYsi2bin",
    "outputId": "2aefbbac-a60e-4d75-fbc1-3f64456ddafc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final result:  [ 8404. 21704.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 8404., 21704.])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(x_train_vec,y_train )\n",
    "comp_toxicity(clf, less_tox_vec, more_tox_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dl5Pr3hG2boV",
    "outputId": "4e11b917-e804-4504-ccb9-921cc3b769d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final result:  [ 8448. 21660.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 8448., 21660.])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(x_train_vec,y_train )\n",
    "comp_toxicity(clf, less_tox_vec, more_tox_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D5qkmADy2bqs",
    "outputId": "ebbd8418-ec6e-410b-8ff2-b45e9ac6d73f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final result:  [ 9085. 21023.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 9085., 21023.])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "clf = BernoulliNB()\n",
    "clf.fit(x_train_vec,y_train )\n",
    "comp_toxicity(clf, less_tox_vec, more_tox_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xkabtg_J2bsx",
    "outputId": "9396100c-97d4-4ad6-af03-f3170b49c04e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final result:  [ 8481. 21627.]\n",
      "final result:  [ 8003. 22105.]\n",
      "final result:  [10310. 19798.]\n",
      "final result:  [ 8481. 21627.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 8481., 21627.])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = 1.0\n",
    "clf = svm.SVC(kernel='linear', C=C, probability = True)\n",
    "clf.fit(x_train_vec,y_train )\n",
    "comp_toxicity(clf, less_tox_vec, more_tox_vec)\n",
    "\n",
    "clf = svm.SVC(kernel='rbf', C=C, probability = True)\n",
    "clf.fit(x_train_vec,y_train )\n",
    "comp_toxicity(clf, less_tox_vec, more_tox_vec)\n",
    "\n",
    "clf = svm.SVC(kernel='sigmoid', C=C, probability = True)\n",
    "clf.fit(x_train_vec,y_train )\n",
    "comp_toxicity(clf, less_tox_vec, more_tox_vec)\n",
    "\n",
    "clf = svm.SVC(kernel='poly', C=C, probability = True)\n",
    "clf.fit(x_train_vec,y_train )\n",
    "comp_toxicity(clf, less_tox_vec, more_tox_vec)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZYo75kf50wzd"
   },
   "source": [
    "# cross validation on different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P5zBWM7YyJYS",
    "outputId": "f07ca3b1-7db5-4b24-f0af-ad460afa2611"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random forest:  0.609096631238619\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=10, random_state=1)\n",
    "cv_scores = cross_val_score(clf, x_train_val_vec, y_train_all, cv=10)\n",
    "\n",
    "print ('random forest: ',cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rRb4DqUuyJYS",
    "outputId": "ad80a73c-2ae1-42db-bb62-1a0ddaf951a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.5512023548502979\n",
      "2 0.5609703073321194\n",
      "3 0.5720559725990065\n",
      "4 0.5686223320574281\n",
      "5 0.5624646958266751\n",
      "6 0.571175613487433\n",
      "7 0.5613207167366556\n",
      "8 0.5728458260557187\n",
      "9 0.5587683643638435\n",
      "10 0.5665985358058643\n",
      "11 0.5599116466609685\n",
      "12 0.5651014375611629\n",
      "13 0.5590337650352423\n",
      "14 0.5629917809407013\n"
     ]
    }
   ],
   "source": [
    "#KNN choosing best neighbour number \n",
    "\n",
    "from sklearn import neighbors\n",
    "\n",
    "\n",
    "for n in range(1, 15):\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors=n)\n",
    "    cv_scores = cross_val_score(clf, x_train_val_vec, y_train_all, cv=10)\n",
    "    print (n, cv_scores.mean())\n",
    "    \n",
    "#k = 8 , 3 are first and sec place for best predictor respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O8g2TuWMyJYS",
    "outputId": "d01dc6a3-90ba-42e6-eeb3-41563dba4bff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5728458260557187"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#KNN \n",
    "\n",
    "from sklearn import neighbors\n",
    "\n",
    "clf = neighbors.KNeighborsClassifier(n_neighbors=8)\n",
    "cv_scores = cross_val_score(clf, x_train_val_vec, y_train_all, cv=10)\n",
    "\n",
    "cv_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hClBlkVlyJYS",
    "outputId": "587386d2-76d1-4ef1-b614-afad2a11b040",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6199204572200131"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "cv_scores = cross_val_score(clf, x_train_val_vec, y_train_all, cv=10)\n",
    "cv_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nx0H3_S_yJYS",
    "outputId": "47099c8d-ca34-4992-af09-4ecb18cf14b1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minmax scaled Baye's:  0.6206232889863367\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes\n",
    "#usually not the best performance, but fast to train\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#scaler = preprocessing.MinMaxScaler()\n",
    "scaler = preprocessing.MaxAbsScaler() #don't really need scaling since the input is already binary \n",
    "training_inputs_minmax = scaler.fit_transform(x_train_val_vec)\n",
    "\n",
    "clf = MultinomialNB()\n",
    "cv_scores = cross_val_score(clf, training_inputs_minmax, y_train_all, cv=10)\n",
    "\n",
    "print ('minmax scaled Baye\\'s: ',cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W0_RUV73yJYS",
    "outputId": "03ccc045-f3cf-4554-8563-098010f8ff48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear kernel 0.6077753570676478\n",
      "rbf kernel 0.6370746466485809\n",
      "sigmoid kernel 0.6134956396249086\n",
      "poly kernel 0.5293857693212928\n"
     ]
    }
   ],
   "source": [
    "#SVM with kernel tuning\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "C = 1.0\n",
    "svc = svm.SVC(kernel='linear', C=C)\n",
    "cv_scores = cross_val_score(svc, x_train_val_vec, y_train_all, cv=10)\n",
    "print ('linear kernel', cv_scores.mean())\n",
    "\n",
    "\n",
    "svc = svm.SVC(kernel='rbf', C=C)\n",
    "cv_scores = cross_val_score(svc, x_train_val_vec, y_train_all, cv=10)\n",
    "cv_scores.mean()\n",
    "print ('rbf kernel', cv_scores.mean())\n",
    "\n",
    "\n",
    "svc = svm.SVC(kernel='sigmoid', C=C)\n",
    "cv_scores = cross_val_score(svc, x_train_val_vec, y_train_all, cv=10)\n",
    "cv_scores.mean()\n",
    "print ('sigmoid kernel', cv_scores.mean())\n",
    "\n",
    "svc = svm.SVC(kernel='poly', C=C)\n",
    "cv_scores = cross_val_score(svc, x_train_val_vec, y_train_all, cv=10)\n",
    "cv_scores.mean()\n",
    "print ('poly kernel', cv_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Jtg47bIyJYS"
   },
   "source": [
    "# try on colab: keras lstm, xgb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jOGyFLqQ0d8s",
    "outputId": "3716f3c2-b0b8-4ef5-f983-c88fc7b3dfec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (0.90)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.19.5)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.4.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W-oSlXPbyJYS",
    "outputId": "9fe17c8f-5648-44a1-9da6-dc28eed54b6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6200051562674198\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "clf = xgb.XGBClassifier( eta = 0.6 ,objective = 'multi:softmax', num_class = 2) #eta higher starts losing acc -- lower is slower\n",
    "cv_scores = cross_val_score(clf, x_train_val_vec, y_train_all, cv=10)\n",
    "print (cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JEidcpSa1qHN"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d_Z2x6uA9NKC"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "sequences_train = tokenizer.texts_to_sequences(x_train)\n",
    "padded_train = sequence.pad_sequences(sequences_train, padding='post', maxlen = 50) \n",
    "\n",
    "sequences_val = tokenizer.texts_to_sequences(x_val)\n",
    "padded_val = sequence.pad_sequences(sequences_val, padding='post', maxlen = 50) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jzA3pSKAGIqn",
    "outputId": "3177fc74-53e6-4e77-9b2e-9f60c4b549ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2841, 50)\n",
      "(8525, 50)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([   36,     8,    39,   324,   115,   217,  5268,    47,   484,\n",
       "         184,     1,   210,    36,    25,    15,    39,  6877,   115,\n",
       "         146,    36,     8,    39,   206, 11772,   178,    33,    30,\n",
       "         115,    38,   255,    24,   118,   307,    39,    64, 11111,\n",
       "           5,   574,    66, 10237,    57,   162,  2130,     0,     0,\n",
       "           0,     0,     0,     0,     0], dtype=int32)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y_train\n",
    "#y_val\n",
    "print(padded_val.shape)\n",
    "print(padded_train.shape)\n",
    "padded_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4blvVGmS1qLs",
    "outputId": "624e33e4-d130-478d-ff65-9a05bb2601a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 128)) \n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2)) #128 intermediate recurrent neurons in the lstm layer // maybe need more or less\n",
    "#single layer 25 does better\n",
    "#model.add(Dense(64, activation='relu'))\n",
    "#model.add(Dense(32, activation='relu'))\n",
    "#model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid')) #sigmoid becuz binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6rDNz3QW1qOU"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy']) #optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FciNPhwRSOQc"
   },
   "outputs": [],
   "source": [
    "my_callbacks = [\n",
    "                EarlyStopping(patience = 3, monitor = 'val_loss'),\n",
    "                ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5') #edit name: as diff parameter changed\n",
    "]\n",
    "\n",
    "# further optimization: https://keras.io/api/callbacks/, https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping#args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UT8wBvJH1qQU",
    "outputId": "7221a930-1717-404c-f86e-d05a430846cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "267/267 - 58s - loss: 0.6894 - accuracy: 0.5390 - val_loss: 0.6675 - val_accuracy: 0.5790 - 58s/epoch - 216ms/step\n",
      "Epoch 2/15\n",
      "267/267 - 55s - loss: 0.6357 - accuracy: 0.6425 - val_loss: 0.6712 - val_accuracy: 0.6054 - 55s/epoch - 205ms/step\n",
      "Epoch 3/15\n",
      "267/267 - 55s - loss: 0.4658 - accuracy: 0.7974 - val_loss: 0.7750 - val_accuracy: 0.6107 - 55s/epoch - 204ms/step\n",
      "Epoch 4/15\n",
      "267/267 - 53s - loss: 0.2933 - accuracy: 0.8887 - val_loss: 0.9297 - val_accuracy: 0.6149 - 53s/epoch - 200ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f71e9705790>"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_train, y_train,\n",
    "          batch_size=32, #num of data to use per step size\n",
    "          epochs=15,\n",
    "          verbose=2,\n",
    "          validation_data=(padded_val, y_val), callbacks = my_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hBmru_snXc1M",
    "outputId": "33306af7-7ff7-4fe1-ce5d-02d0cef5224f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_7 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "#best model from callback \n",
    "from tensorflow import keras\n",
    "model = keras.models.load_model('model.02-0.62.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CXru6_p2X_0R"
   },
   "outputs": [],
   "source": [
    "#pad the test set prediction: 7:35 from https://www.youtube.com/watch?v=Y_hzMnRXjhI\n",
    "\n",
    "\n",
    "#model prediction on the padded test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "He-KQcMvX_7j"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BY6BvAYcYAIE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wyJHNmRULdlo"
   },
   "source": [
    "# accuracy still very bad: consider grabbing other dataset to train OR adding more dense layers with the bigger dataset\n",
    "\n",
    "## trying with different maxlen in lstm \n",
    "was using padding maxlen = 20, 59.87%\n",
    "maxlen = 80, got about 55%\n",
    "maxlen = 50, got 55.79%\n",
    "\n",
    "## adding dense intermediate layers with maxlen = 50 \n",
    "model.add(Dense(25, activation='relu')) // 61%\n",
    "\n",
    "//not as good: 60.47%\n",
    "model.add(Dense(90, activation='relu'))\n",
    "model.add(Dense(60, activation='relu'))\n",
    "model.add(Dense(24, activation='relu'))\n",
    "\n",
    "//60.15%\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "\n",
    "//61.46%, 60.08\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "\n",
    "//60.65%\n",
    "model.add(Dense(16, activation='relu'))\n",
    "\n",
    "//60.26%\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "\n",
    "//59%\n",
    "model.add(Dense(8, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l5GB-FkZX_Dl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H-qWPX4DX_Gb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VcYuPMyRLckB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ch7QklDnLcmY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bjUfk1_-1qSZ"
   },
   "outputs": [],
   "source": [
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=32,\n",
    "                            verbose=2)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "bow.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "kaggle_kernel",
   "language": "python",
   "name": "kaggle_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
