{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KnUPQ03ozgN6"
   },
   "source": [
    "# Comment Toxicity Severity Rater\n",
    "- group by the same comment and find out if that comment is more often rated as more/ less toxic \n",
    "- transform the input data to be more / less toxic 0 and 1 // train on these\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IR61GQ1Qywzb",
    "outputId": "67ef639b-e210-4526-ff0f-b484165995f4"
   },
   "outputs": [],
   "source": [
    "#!pip install alt-profanity-check\n",
    "!pip install joblib\n",
    "!pip install scikit-learn\n",
    "#install tweet-preprocessor to clean tweets\n",
    "!pip install tweet-preprocessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JK2ClqlvzPs7"
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YBi80m2m0wO4"
   },
   "outputs": [],
   "source": [
    "# remove special characters using the regular expression library\n",
    "# updated with . and = \n",
    "import re\n",
    "\n",
    "#set up punctuations we want to be replaced\n",
    "REPLACE_NO_SPACE = re.compile(\"(\\*)|(\\=\\=)|(\\~) | (\\=) | (\\.\\.\\.) |(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\|)|(\\()|(\\))|(\\[)|(\\])|(\\%)|(\\$)|(\\>)|(\\<)|(\\{)|(\\})\")\n",
    "REPLACE_WITH_SPACE = re.compile(\" (<br\\s/><br\\s/?)| (\\n\\n) | (\\n) |(\\.) |(-)|(/)|(:). \")\n",
    "\n",
    "#re.escape (* )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# re.sub() // natalie uses this more often\n",
    "\n",
    "# regex has a method to escape the char properly \n",
    "re.escape\n",
    "\n",
    "\n",
    "# gridsearchcv() // can be slow \n",
    "if have a few val, can use \n",
    "\n",
    "# use randomsearch() // possible range // faster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "NsFs8bzc0ylC"
   },
   "outputs": [],
   "source": [
    "import preprocessor as p\n",
    "\n",
    "# custum function to clean the dataset (combining tweet_preprocessor and reguar expression)\n",
    "def clean_tweets(df):\n",
    "  tempArr = []\n",
    "  for line in df:\n",
    "    # send to tweet_processor\n",
    "    tmpL = p.clean(line)\n",
    "    # remove puctuation\n",
    "    tmpL = REPLACE_NO_SPACE.sub(\"\", tmpL.lower()) # convert all tweets to lower cases\n",
    "    tmpL = REPLACE_WITH_SPACE.sub(\" \", tmpL)\n",
    "    tempArr.append(tmpL)\n",
    "  return tempArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "jdxdQ2Fz00L3"
   },
   "outputs": [],
   "source": [
    "original_data = pd.read_csv(\"./data/validation_data.csv\")\n",
    "\n",
    "data_to_test = pd.read_csv('./data/comments_to_score.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "Jcgva1U5yJYN",
    "outputId": "478c0002-a327-4233-a62c-c62d16907adf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>worker</th>\n",
       "      <th>less_toxic</th>\n",
       "      <th>more_toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>313</td>\n",
       "      <td>This article sucks \\n\\nwoo woo wooooooo</td>\n",
       "      <td>WHAT!!!!!!!!?!?!!?!?!!?!?!?!?!!!!!!!!!!!!!!!!!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>188</td>\n",
       "      <td>\"And yes, people should recognize that but the...</td>\n",
       "      <td>Daphne Guinness \\n\\nTop of the mornin' my fav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>82</td>\n",
       "      <td>Western Media?\\n\\nYup, because every crime in...</td>\n",
       "      <td>\"Atom you don't believe actual photos of mastu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>347</td>\n",
       "      <td>And you removed it! You numbskull! I don't car...</td>\n",
       "      <td>You seem to have sand in your vagina.\\n\\nMight...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>539</td>\n",
       "      <td>smelly vagina \\n\\nBluerasberry why don't you ...</td>\n",
       "      <td>hey \\n\\nway to support nazis, you racist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   worker                                         less_toxic  \\\n",
       "0     313            This article sucks \\n\\nwoo woo wooooooo   \n",
       "1     188  \"And yes, people should recognize that but the...   \n",
       "2      82   Western Media?\\n\\nYup, because every crime in...   \n",
       "3     347  And you removed it! You numbskull! I don't car...   \n",
       "4     539   smelly vagina \\n\\nBluerasberry why don't you ...   \n",
       "\n",
       "                                          more_toxic  \n",
       "0  WHAT!!!!!!!!?!?!!?!?!!?!?!?!?!!!!!!!!!!!!!!!!!...  \n",
       "1   Daphne Guinness \\n\\nTop of the mornin' my fav...  \n",
       "2  \"Atom you don't believe actual photos of mastu...  \n",
       "3  You seem to have sand in your vagina.\\n\\nMight...  \n",
       "4           hey \\n\\nway to support nazis, you racist  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "original_data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "5beXEglKyJYO",
    "outputId": "212d55c9-b984-47ed-dd95-8538a49ce5d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WHAT!!!!!!!!?!?!!?!?!!?!?!?!?!!!!!!!!!!!!!!!!!!!!!!!!??????????????????????????????????????????????????????!!!!!!=\\nWHER IS YOUR SEXY PIC GONE FROM YOUR MAIN PAGE???????? put it back'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#original_data.iloc[0]\n",
    "original_data['more_toxic'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "4Ned_u-3yJYO",
    "outputId": "d01310d0-1d89-4cb0-d6f4-41fb292e9213"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>worker</th>\n",
       "      <th>less_toxic</th>\n",
       "      <th>more_toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>313</td>\n",
       "      <td>This article sucks \\n\\nwoo woo wooooooo</td>\n",
       "      <td>WHAT!!!!!!!!?!?!!?!?!!?!?!?!?!!!!!!!!!!!!!!!!!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>188</td>\n",
       "      <td>\"And yes, people should recognize that but the...</td>\n",
       "      <td>Daphne Guinness \\n\\nTop of the mornin' my fav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>82</td>\n",
       "      <td>Western Media?\\n\\nYup, because every crime in...</td>\n",
       "      <td>\"Atom you don't believe actual photos of mastu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>347</td>\n",
       "      <td>And you removed it! You numbskull! I don't car...</td>\n",
       "      <td>You seem to have sand in your vagina.\\n\\nMight...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>539</td>\n",
       "      <td>smelly vagina \\n\\nBluerasberry why don't you ...</td>\n",
       "      <td>hey \\n\\nway to support nazis, you racist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30103</th>\n",
       "      <td>461</td>\n",
       "      <td>I'm sorry. I'm not an admin. I will give you t...</td>\n",
       "      <td>get out my large penis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30104</th>\n",
       "      <td>527</td>\n",
       "      <td>I'm sorry. I'm not an admin. I will give you t...</td>\n",
       "      <td>get out my large penis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30105</th>\n",
       "      <td>352</td>\n",
       "      <td>wow...\\nare you out of your mind, how was my e...</td>\n",
       "      <td>Piss off you slant eyed-gook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30106</th>\n",
       "      <td>311</td>\n",
       "      <td>wow...\\nare you out of your mind, how was my e...</td>\n",
       "      <td>Piss off you slant eyed-gook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30107</th>\n",
       "      <td>54</td>\n",
       "      <td>wow...\\nare you out of your mind, how was my e...</td>\n",
       "      <td>Piss off you slant eyed-gook</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30108 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       worker                                         less_toxic  \\\n",
       "0         313            This article sucks \\n\\nwoo woo wooooooo   \n",
       "1         188  \"And yes, people should recognize that but the...   \n",
       "2          82   Western Media?\\n\\nYup, because every crime in...   \n",
       "3         347  And you removed it! You numbskull! I don't car...   \n",
       "4         539   smelly vagina \\n\\nBluerasberry why don't you ...   \n",
       "...       ...                                                ...   \n",
       "30103     461  I'm sorry. I'm not an admin. I will give you t...   \n",
       "30104     527  I'm sorry. I'm not an admin. I will give you t...   \n",
       "30105     352  wow...\\nare you out of your mind, how was my e...   \n",
       "30106     311  wow...\\nare you out of your mind, how was my e...   \n",
       "30107      54  wow...\\nare you out of your mind, how was my e...   \n",
       "\n",
       "                                              more_toxic  \n",
       "0      WHAT!!!!!!!!?!?!!?!?!!?!?!?!?!!!!!!!!!!!!!!!!!...  \n",
       "1       Daphne Guinness \\n\\nTop of the mornin' my fav...  \n",
       "2      \"Atom you don't believe actual photos of mastu...  \n",
       "3      You seem to have sand in your vagina.\\n\\nMight...  \n",
       "4               hey \\n\\nway to support nazis, you racist  \n",
       "...                                                  ...  \n",
       "30103                             get out my large penis  \n",
       "30104                             get out my large penis  \n",
       "30105                       Piss off you slant eyed-gook  \n",
       "30106                       Piss off you slant eyed-gook  \n",
       "30107                       Piss off you slant eyed-gook  \n",
       "\n",
       "[30108 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data = original_data\n",
    "cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3ynSbwXpyJYP"
   },
   "outputs": [],
   "source": [
    "clean_less_toxic = clean_tweets(original_data[\"less_toxic\"])\n",
    "cleaned_data['less_toxic'] = pd.DataFrame(clean_less_toxic)\n",
    "#cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "JUnN_J62yJYP",
    "outputId": "cc219ae9-801f-4413-e66b-179801803916"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>worker</th>\n",
       "      <th>less_toxic</th>\n",
       "      <th>more_toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>313</td>\n",
       "      <td>this article sucks woo woo wooooooo</td>\n",
       "      <td>what=wher is your sexy pic gone from your main...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>188</td>\n",
       "      <td>and yes people should recognize that but they ...</td>\n",
       "      <td>daphne guinness top of the mornin my favourite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>82</td>\n",
       "      <td>western mediayup because every crime in the en...</td>\n",
       "      <td>atom you dont believe actual photos of masturb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>347</td>\n",
       "      <td>and you removed it you numbskull i dont care w...</td>\n",
       "      <td>you seem to have sand in your vagina.might wan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>539</td>\n",
       "      <td>smelly vagina bluerasberry why dont you be a m...</td>\n",
       "      <td>hey way to support nazis you racist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30103</th>\n",
       "      <td>461</td>\n",
       "      <td>im sorry im not an admin i will give you three...</td>\n",
       "      <td>get out my large penis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30104</th>\n",
       "      <td>527</td>\n",
       "      <td>im sorry im not an admin i will give you three...</td>\n",
       "      <td>get out my large penis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30105</th>\n",
       "      <td>352</td>\n",
       "      <td>wow...are you out of your mind how was my edit...</td>\n",
       "      <td>piss off you slant eyed gook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30106</th>\n",
       "      <td>311</td>\n",
       "      <td>wow...are you out of your mind how was my edit...</td>\n",
       "      <td>piss off you slant eyed gook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30107</th>\n",
       "      <td>54</td>\n",
       "      <td>wow...are you out of your mind how was my edit...</td>\n",
       "      <td>piss off you slant eyed gook</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30108 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       worker                                         less_toxic  \\\n",
       "0         313                this article sucks woo woo wooooooo   \n",
       "1         188  and yes people should recognize that but they ...   \n",
       "2          82  western mediayup because every crime in the en...   \n",
       "3         347  and you removed it you numbskull i dont care w...   \n",
       "4         539  smelly vagina bluerasberry why dont you be a m...   \n",
       "...       ...                                                ...   \n",
       "30103     461  im sorry im not an admin i will give you three...   \n",
       "30104     527  im sorry im not an admin i will give you three...   \n",
       "30105     352  wow...are you out of your mind how was my edit...   \n",
       "30106     311  wow...are you out of your mind how was my edit...   \n",
       "30107      54  wow...are you out of your mind how was my edit...   \n",
       "\n",
       "                                              more_toxic  \n",
       "0      what=wher is your sexy pic gone from your main...  \n",
       "1      daphne guinness top of the mornin my favourite...  \n",
       "2      atom you dont believe actual photos of masturb...  \n",
       "3      you seem to have sand in your vagina.might wan...  \n",
       "4                    hey way to support nazis you racist  \n",
       "...                                                  ...  \n",
       "30103                             get out my large penis  \n",
       "30104                             get out my large penis  \n",
       "30105                       piss off you slant eyed gook  \n",
       "30106                       piss off you slant eyed gook  \n",
       "30107                       piss off you slant eyed gook  \n",
       "\n",
       "[30108 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_more_toxic = clean_tweets(original_data[\"more_toxic\"])\n",
    "cleaned_data['more_toxic'] = pd.DataFrame(clean_more_toxic)\n",
    "cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what=wher is your sexy pic gone from your main page put it back'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " cleaned_data['more_toxic'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col_0</th>\n",
       "      <th>col_1</th>\n",
       "      <th>more_toxic_col_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this article sucks woo woo wooooooo</td>\n",
       "      <td>what=wher is your sexy pic gone from your main...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and yes people should recognize that but they ...</td>\n",
       "      <td>daphne guinness top of the mornin my favourite...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>atom you dont believe actual photos of masturb...</td>\n",
       "      <td>western mediayup because every crime in the en...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>you seem to have sand in your vagina.might wan...</td>\n",
       "      <td>and you removed it you numbskull i dont care w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>smelly vagina bluerasberry why dont you be a m...</td>\n",
       "      <td>hey way to support nazis you racist</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30103</th>\n",
       "      <td>get out my large penis</td>\n",
       "      <td>im sorry im not an admin i will give you three...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30104</th>\n",
       "      <td>im sorry im not an admin i will give you three...</td>\n",
       "      <td>get out my large penis</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30105</th>\n",
       "      <td>piss off you slant eyed gook</td>\n",
       "      <td>wow...are you out of your mind how was my edit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30106</th>\n",
       "      <td>piss off you slant eyed gook</td>\n",
       "      <td>wow...are you out of your mind how was my edit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30107</th>\n",
       "      <td>piss off you slant eyed gook</td>\n",
       "      <td>wow...are you out of your mind how was my edit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30108 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   col_0  \\\n",
       "0                    this article sucks woo woo wooooooo   \n",
       "1      and yes people should recognize that but they ...   \n",
       "2      atom you dont believe actual photos of masturb...   \n",
       "3      you seem to have sand in your vagina.might wan...   \n",
       "4      smelly vagina bluerasberry why dont you be a m...   \n",
       "...                                                  ...   \n",
       "30103                             get out my large penis   \n",
       "30104  im sorry im not an admin i will give you three...   \n",
       "30105                       piss off you slant eyed gook   \n",
       "30106                       piss off you slant eyed gook   \n",
       "30107                       piss off you slant eyed gook   \n",
       "\n",
       "                                                   col_1  more_toxic_col_idx  \n",
       "0      what=wher is your sexy pic gone from your main...                   1  \n",
       "1      daphne guinness top of the mornin my favourite...                   1  \n",
       "2      western mediayup because every crime in the en...                   0  \n",
       "3      and you removed it you numbskull i dont care w...                   0  \n",
       "4                    hey way to support nazis you racist                   1  \n",
       "...                                                  ...                 ...  \n",
       "30103  im sorry im not an admin i will give you three...                   0  \n",
       "30104                             get out my large penis                   1  \n",
       "30105  wow...are you out of your mind how was my edit...                   0  \n",
       "30106  wow...are you out of your mind how was my edit...                   0  \n",
       "30107  wow...are you out of your mind how was my edit...                   0  \n",
       "\n",
       "[30108 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data_shuffle = pd.DataFrame()\n",
    "indices_moreToxic = []\n",
    "col_1 = []\n",
    "col_0 = []\n",
    "random.seed(0)\n",
    "\n",
    "for i in range(cleaned_data.shape[0]):\n",
    "    idx = round(random.uniform(0, 1))\n",
    "    #print('index of more toxic',idx)\n",
    "    \n",
    "    indices_moreToxic.append(idx)#index of where the more toxic comment is \n",
    "        #1 for right column, 0 for left column \n",
    "    \n",
    "    if idx == 1: \n",
    "        col_1.append( cleaned_data['more_toxic'][i])\n",
    "        col_0.append( cleaned_data['less_toxic'][i])\n",
    "        \n",
    "    else:\n",
    "        col_0.append(cleaned_data['more_toxic'][i])\n",
    "        col_1.append( cleaned_data['less_toxic'][i])\n",
    "\n",
    "\n",
    "cleaned_data_shuffle['col_0'] = pd.Series(col_0)\n",
    "#trans = pd.DataFrame(comment, columns = ['comment']) // another method of setting to df col\n",
    "\n",
    "cleaned_data_shuffle['col_1'] = pd.Series(col_1)\n",
    "cleaned_data_shuffle['more_toxic_col_idx'] = pd.DataFrame(indices_moreToxic)\n",
    "\n",
    "cleaned_data_shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cleaned_data_shuffle.col_0.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "T4rexB1zyJYS",
    "outputId": "ea696975-fc17-47a4-9edf-9b8f7522c710",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#clean the input for predition data\n",
    "cleaned_text = clean_tweets(data_to_test['text'])\n",
    "\n",
    "data_to_test['text'] = pd.DataFrame(cleaned_text)\n",
    "#data_to_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "uVP8iy3t6qtG",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extract the labels from the train data\n",
    "#y = trans.score.values\n",
    "#X = trans.comment.values\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# initalize vectorizer: vectorize tweets for model building\n",
    "vectorizer = CountVectorizer(binary=True, stop_words='english') #stop_words removed: a, the\n",
    "\n",
    "# learn a vocabulary dictionary of all tokens in the raw documents\n",
    "vectorizer.fit(list(cleaned_data_shuffle.col_0.values) + \n",
    "               list(cleaned_data_shuffle.col_1.values) + list(data_to_test.text.values))\n",
    "    \n",
    "\n",
    "# transform documents to document-term matrix\n",
    "col0_vec = vectorizer.transform(cleaned_data_shuffle.col_0)\n",
    "col1_vec = vectorizer.transform(cleaned_data_shuffle.col_1)\n",
    "comments_2_score = vectorizer.transform(data_to_test[\"text\"].values)\n",
    "\n",
    "# combine col0 and col1\n",
    "col0_1_vec = vectorizer.transform(cleaned_data_shuffle.col_0 + ' ' + cleaned_data_shuffle.col_1)\n",
    "\n",
    "\n",
    "# for testing on the original clean more and less toxic col (unshuffled)\n",
    "more_tox_vec = vectorizer.transform(cleaned_data['more_toxic'].values)\n",
    "less_tox_vec = vectorizer.transform(cleaned_data['less_toxic'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_vec = vectorizer.transform([' ']*30108)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<30108x46974 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 0 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_features = np.hstack((col0_vec, col1_vec)) # not the right type of concatenation\n",
    "#input_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<30108x93948 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1508591 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.sparse as sp\n",
    "\n",
    "combined_2 = sp.hstack([col0_vec, col1_vec], format='csr')\n",
    "combined_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6 * int(X[0].shape[0] /10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6*int(X[0].shape[0]/10)+ int(X[0].shape[0]/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(X[0].shape[0]/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0][0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = X[:][6 * int(X[0].shape[0]/10) : 6 * int(X[0].shape[0]/10) + int(X[0].shape[0]/5),]\n",
    "x_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = X[:][:][:][1]\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c83rXllE12qD"
   },
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# extract the labels from the train data\n",
    "y = cleaned_data_shuffle.more_toxic_col_idx.values\n",
    "X = input_features\n",
    "\n",
    "\n",
    "#valid\n",
    "x_val = X[:][6 * int(X[0].shape[0] /10):6 * int(X[0].shape[0]/10)+ int(X[0].shape[0]/5),]\n",
    "y_val = y[6*int(y.shape[0]/10):6*int(y.shape[0]/10) +\n",
    "          int(y.shape[0]/5),]\n",
    "# train\n",
    "x_train = X[:][:6 * int(X[0].shape[0] /10),]\n",
    "y_train = y[:6 * int(y.shape[0] /10),]\n",
    "\n",
    "\n",
    "\n",
    "#test\n",
    "x_test = X[:][6 * int(X[0].shape[0]/10)+ int(X[0].shape[0]/5):,]\n",
    "y_test = y[6*int(y.shape[0]/10) +\n",
    "          int(y.shape[0]/5):,]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# concatenate on the last dim\n",
    "# same rows, vec from first one on the sec vec\n",
    "\n",
    "# TODO: LOOK AT SLACK WITH NATALIE FOR NORMAL MODEL TRAIN 2) LISTEN TO THE PHONE RECORDING FOR LSTM AND LOGISTIC REGRESSION TRAIN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 features ? \n",
    "\n",
    "for part 1 and part 2? \n",
    "\n",
    "## concatenate the whole vector count\n",
    "\n",
    "# but how to tell the model that feature 1 of column 1 is for lesser -- how to put a label on that ? when the same row has another column for more toxic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fiI0nmfv05md"
   },
   "source": [
    "# comparing the classification accuracy of basic models // should place after the cross-validated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "ZKNXtETtcb81"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import rankdata\n",
    "# make a function to score the prediction and compare if the \"lesser\" toxic rated is lower prob of rated toxicity\n",
    "def comp_toxicity (model, less_tox_val_vec, more_tox_val_vec):\n",
    "    '''\n",
    "    model is the trained model to be evaluated against the less and more toxic comments\n",
    "    objective is to see if more toxic comments have a higher score\n",
    "\n",
    "    less_tox_val_vec and more_tox_val_vec are the respective column in dataframe that contains the comments vectorized\n",
    "    '''\n",
    "    result = res = np.zeros((1,2))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for i in range(cleaned_data.shape[0]):\n",
    "        result = rankdata([model.predict_proba(less_tox_val_vec[i]).tolist()[0][1], \n",
    "                 model.predict_proba(more_tox_val_vec[i]).tolist()[0][1]], method = 'ordinal' )\n",
    "        result -= 1\n",
    "        #print(result)\n",
    "        res += result\n",
    "        #print(res[0])\n",
    "        '''\n",
    "        if i == 5:\n",
    "            break\n",
    "        '''\n",
    "    print ('final result: ',res[0])\n",
    "    return res[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bxFOh2JRkELg",
    "outputId": "758aff87-be81-4a6d-c52e-7da477e1eedc"
   },
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "clf = RandomForestClassifier(n_estimators=10, random_state=1)\n",
    "clf.fit(col0_1_vec, cleaned_data_shuffle.more_toxic_col_idx )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w9vJ8s2gwE-T",
    "outputId": "238fff8c-0339-4c05-de40-8abcb010e8b6"
   },
   "outputs": [],
   "source": [
    "comp_toxicity(clf, less_tox_vec, more_tox_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bxFOh2JRkELg",
    "outputId": "758aff87-be81-4a6d-c52e-7da477e1eedc"
   },
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "clf = RandomForestClassifier(n_estimators=10, random_state=1)\n",
    "clf.fit(input_features, cleaned_data_shuffle.more_toxic_col_idx )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w9vJ8s2gwE-T",
    "outputId": "238fff8c-0339-4c05-de40-8abcb010e8b6"
   },
   "outputs": [],
   "source": [
    "comp_toxicity(clf, less_tox_vec, more_tox_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U8KtIcgUrLd5",
    "outputId": "5979915e-721e-4552-8ce4-f77f71a265a0"
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "clf = neighbors.KNeighborsClassifier(n_neighbors=3)\n",
    "clf.fit(col0_vec + col1_vec, cleaned_data_shuffle.more_toxic_col_idx )\n",
    "comp_toxicity(clf, less_tox_vec, more_tox_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<30108x93948 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1601398 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "less_tox_repeat_vec = sp.hstack([less_tox_vec, less_tox_vec], format='csr')\n",
    "less_tox_repeat_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<30108x93948 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1415784 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "more_tox_repeat_vec = sp.hstack([more_tox_vec, more_tox_vec], format='csr')\n",
    "more_tox_repeat_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<30108x93948 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 800699 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "less_tox_empty_vec = sp.hstack([empty_vec, less_tox_vec], format='csr') \n",
    "# need empty_vec on the left side, comp_toxicity assumes more toxic on the right -- toxicity prob score\n",
    "less_tox_empty_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<30108x93948 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 707892 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "more_tox_empty_vec = sp.hstack([ empty_vec, more_tox_vec], format='csr')\n",
    "more_tox_empty_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8CYNXYsi2bin",
    "outputId": "2aefbbac-a60e-4d75-fbc1-3f64456ddafc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ericling/Documents/GitHub/DataSci Projects/KaggleToxicity/Kaggle_ToxicityRater/kaggle_kernel/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final result:  [ 7725. 22383.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 7725., 22383.])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(combined_2, cleaned_data_shuffle.more_toxic_col_idx )\n",
    "comp_toxicity(clf, less_tox_empty_vec , more_tox_empty_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ericling/Documents/GitHub/DataSci Projects/KaggleToxicity/Kaggle_ToxicityRater/kaggle_kernel/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.42712499, 0.57287501],\n",
       "       [0.61437132, 0.38562868],\n",
       "       [0.66234533, 0.33765467],\n",
       "       [0.71174425, 0.28825575],\n",
       "       [0.39100691, 0.60899309],\n",
       "       [0.78307489, 0.21692511]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(combined_2, cleaned_data_shuffle.more_toxic_col_idx )\n",
    "\n",
    "\n",
    "clf.predict_proba(less_tox_empty_vec[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.4284193843862787, 0.5715806156137213],\n",
       " [0.553597686196861, 0.44640231380313894],\n",
       " [0.8605638169198045, 0.13943618308019542],\n",
       " [0.8790218005259434, 0.12097819947405668],\n",
       " [0.8164261106112024, 0.1835738893887976],\n",
       " [0.4977800759680051, 0.5022199240319949]]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict_proba(more_tox_empty_vec[:6]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    22385\n",
       "0     7723\n",
       "dtype: int64"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(clf.predict(more_tox_empty_vec)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    17811\n",
       "1    12297\n",
       "dtype: int64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(clf.predict(less_tox_empty_vec)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "summed_more_tox_is_more_tox = 0\n",
    "for i in range(30108):\n",
    "    summed_more_tox_is_more_tox = clf.predict_proba(more_tox_empty_vec)[i][0].sum()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8CYNXYsi2bin",
    "outputId": "2aefbbac-a60e-4d75-fbc1-3f64456ddafc"
   },
   "source": [
    "#Logistic Regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(combined_2, cleaned_data_shuffle.more_toxic_col_idx )\n",
    "comp_toxicity(clf, less_tox_repeat_vec , more_tox_repeat_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(combined_2, cleaned_data_shuffle.more_toxic_col_idx )\n",
    "\n",
    "\n",
    "clf.predict_proba(less_tox_repeat_vec[:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(combined_2, cleaned_data_shuffle.more_toxic_col_idx )\n",
    "\n",
    "\n",
    "clf.predict_proba(more_tox_repeat_vec[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final result:  [21593.  8515.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([21593.,  8515.])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(combined_2, cleaned_data_shuffle.more_toxic_col_idx )\n",
    "comp_toxicity(clf, less_tox_empty_vec, more_tox_empty_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dl5Pr3hG2boV",
    "outputId": "4e11b917-e804-4504-ccb9-921cc3b769d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final result:  [15231. 14877.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([15231., 14877.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(col0_1_vec, cleaned_data_shuffle.more_toxic_col_idx )\n",
    "comp_toxicity(clf, less_tox_vec, more_tox_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D5qkmADy2bqs",
    "outputId": "ebbd8418-ec6e-410b-8ff2-b45e9ac6d73f"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "clf = BernoulliNB()\n",
    "clf.fit(col0_1_vec, cleaned_data_shuffle.more_toxic_col_idx )\n",
    "comp_toxicity(clf, less_tox_vec, more_tox_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xkabtg_J2bsx",
    "outputId": "9396100c-97d4-4ad6-af03-f3170b49c04e"
   },
   "outputs": [],
   "source": [
    "C = 1.0\n",
    "clf = svm.SVC(kernel='linear', C=C, probability = True)\n",
    "clf.fit(x_train_vec,y_train )\n",
    "comp_toxicity(clf, less_tox_vec, more_tox_vec)\n",
    "\n",
    "clf = svm.SVC(kernel='rbf', C=C, probability = True)\n",
    "clf.fit(x_train_vec,y_train )\n",
    "comp_toxicity(clf, less_tox_vec, more_tox_vec)\n",
    "\n",
    "clf = svm.SVC(kernel='sigmoid', C=C, probability = True)\n",
    "clf.fit(x_train_vec,y_train )\n",
    "comp_toxicity(clf, less_tox_vec, more_tox_vec)\n",
    "\n",
    "clf = svm.SVC(kernel='poly', C=C, probability = True)\n",
    "clf.fit(x_train_vec,y_train )\n",
    "comp_toxicity(clf, less_tox_vec, more_tox_vec)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZYo75kf50wzd"
   },
   "source": [
    "# cross validation on different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P5zBWM7YyJYS",
    "outputId": "f07ca3b1-7db5-4b24-f0af-ad460afa2611"
   },
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=10, random_state=1)\n",
    "cv_scores = cross_val_score(clf, x_train_val_vec, y_train_all, cv=10)\n",
    "\n",
    "print ('random forest: ',cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rRb4DqUuyJYS",
    "outputId": "ad80a73c-2ae1-42db-bb62-1a0ddaf951a2"
   },
   "outputs": [],
   "source": [
    "#KNN choosing best neighbour number \n",
    "\n",
    "from sklearn import neighbors\n",
    "\n",
    "\n",
    "for n in range(1, 15):\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors=n)\n",
    "    cv_scores = cross_val_score(clf, x_train_val_vec, y_train_all, cv=10)\n",
    "    print (n, cv_scores.mean())\n",
    "    \n",
    "#k = 8 , 3 are first and sec place for best predictor respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O8g2TuWMyJYS",
    "outputId": "d01dc6a3-90ba-42e6-eeb3-41563dba4bff"
   },
   "outputs": [],
   "source": [
    "#KNN \n",
    "\n",
    "from sklearn import neighbors\n",
    "\n",
    "clf = neighbors.KNeighborsClassifier(n_neighbors=8)\n",
    "cv_scores = cross_val_score(clf, x_train_val_vec, y_train_all, cv=10)\n",
    "\n",
    "cv_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hClBlkVlyJYS",
    "outputId": "587386d2-76d1-4ef1-b614-afad2a11b040",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "cv_scores = cross_val_score(clf, x_train_val_vec, y_train_all, cv=10)\n",
    "cv_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nx0H3_S_yJYS",
    "outputId": "47099c8d-ca34-4992-af09-4ecb18cf14b1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Naive Bayes\n",
    "#usually not the best performance, but fast to train\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#scaler = preprocessing.MinMaxScaler()\n",
    "scaler = preprocessing.MaxAbsScaler() #don't really need scaling since the input is already binary \n",
    "training_inputs_minmax = scaler.fit_transform(x_train_val_vec)\n",
    "\n",
    "clf = MultinomialNB()\n",
    "cv_scores = cross_val_score(clf, training_inputs_minmax, y_train_all, cv=10)\n",
    "\n",
    "print ('minmax scaled Baye\\'s: ',cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W0_RUV73yJYS",
    "outputId": "03ccc045-f3cf-4554-8563-098010f8ff48"
   },
   "outputs": [],
   "source": [
    "#SVM with kernel tuning\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "C = 1.0\n",
    "svc = svm.SVC(kernel='linear', C=C)\n",
    "cv_scores = cross_val_score(svc, x_train_val_vec, y_train_all, cv=10)\n",
    "print ('linear kernel', cv_scores.mean())\n",
    "\n",
    "\n",
    "svc = svm.SVC(kernel='rbf', C=C)\n",
    "cv_scores = cross_val_score(svc, x_train_val_vec, y_train_all, cv=10)\n",
    "cv_scores.mean()\n",
    "print ('rbf kernel', cv_scores.mean())\n",
    "\n",
    "\n",
    "svc = svm.SVC(kernel='sigmoid', C=C)\n",
    "cv_scores = cross_val_score(svc, x_train_val_vec, y_train_all, cv=10)\n",
    "cv_scores.mean()\n",
    "print ('sigmoid kernel', cv_scores.mean())\n",
    "\n",
    "svc = svm.SVC(kernel='poly', C=C)\n",
    "cv_scores = cross_val_score(svc, x_train_val_vec, y_train_all, cv=10)\n",
    "cv_scores.mean()\n",
    "print ('poly kernel', cv_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Jtg47bIyJYS"
   },
   "source": [
    "# try on colab: keras lstm, xgb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jOGyFLqQ0d8s",
    "outputId": "3716f3c2-b0b8-4ef5-f983-c88fc7b3dfec"
   },
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W-oSlXPbyJYS",
    "outputId": "9fe17c8f-5648-44a1-9da6-dc28eed54b6e"
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "clf = xgb.XGBClassifier( eta = 0.6 ,objective = 'multi:softmax', num_class = 2) #eta higher starts losing acc -- lower is slower\n",
    "cv_scores = cross_val_score(clf, x_train_val_vec, y_train_all, cv=10)\n",
    "print (cv_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JEidcpSa1qHN"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d_Z2x6uA9NKC"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "sequences_train = tokenizer.texts_to_sequences(x_train)\n",
    "padded_train = sequence.pad_sequences(sequences_train, padding='post', maxlen = 50) \n",
    "\n",
    "sequences_val = tokenizer.texts_to_sequences(x_val)\n",
    "padded_val = sequence.pad_sequences(sequences_val, padding='post', maxlen = 50) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jzA3pSKAGIqn",
    "outputId": "3177fc74-53e6-4e77-9b2e-9f60c4b549ac"
   },
   "outputs": [],
   "source": [
    "#y_train\n",
    "#y_val\n",
    "print(padded_val.shape)\n",
    "print(padded_train.shape)\n",
    "padded_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4blvVGmS1qLs",
    "outputId": "624e33e4-d130-478d-ff65-9a05bb2601a3"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 128)) \n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2)) #128 intermediate recurrent neurons in the lstm layer // maybe need more or less\n",
    "#single layer 25 does better\n",
    "#model.add(Dense(64, activation='relu'))\n",
    "#model.add(Dense(32, activation='relu'))\n",
    "#model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid')) #sigmoid becuz binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6rDNz3QW1qOU"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy']) #optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FciNPhwRSOQc"
   },
   "outputs": [],
   "source": [
    "my_callbacks = [\n",
    "                EarlyStopping(patience = 3, monitor = 'val_loss'),\n",
    "                ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5') #edit name: as diff parameter changed\n",
    "]\n",
    "\n",
    "# further optimization: https://keras.io/api/callbacks/, https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping#args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UT8wBvJH1qQU",
    "outputId": "7221a930-1717-404c-f86e-d05a430846cc"
   },
   "outputs": [],
   "source": [
    "model.fit(padded_train, y_train,\n",
    "          batch_size=32, #num of data to use per step size\n",
    "          epochs=15,\n",
    "          verbose=2,\n",
    "          validation_data=(padded_val, y_val), callbacks = my_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hBmru_snXc1M",
    "outputId": "33306af7-7ff7-4fe1-ce5d-02d0cef5224f"
   },
   "outputs": [],
   "source": [
    "#best model from callback \n",
    "from tensorflow import keras\n",
    "model = keras.models.load_model('model.02-0.62.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CXru6_p2X_0R"
   },
   "outputs": [],
   "source": [
    "#pad the test set prediction: 7:35 from https://www.youtube.com/watch?v=Y_hzMnRXjhI\n",
    "\n",
    "\n",
    "#model prediction on the padded test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "He-KQcMvX_7j"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BY6BvAYcYAIE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wyJHNmRULdlo"
   },
   "source": [
    "# accuracy still very bad: consider grabbing other dataset to train OR adding more dense layers with the bigger dataset\n",
    "\n",
    "## trying with different maxlen in lstm \n",
    "was using padding maxlen = 20, 59.87%\n",
    "maxlen = 80, got about 55%\n",
    "maxlen = 50, got 55.79%\n",
    "\n",
    "## adding dense intermediate layers with maxlen = 50 \n",
    "model.add(Dense(25, activation='relu')) // 61%\n",
    "\n",
    "//not as good: 60.47%\n",
    "model.add(Dense(90, activation='relu'))\n",
    "model.add(Dense(60, activation='relu'))\n",
    "model.add(Dense(24, activation='relu'))\n",
    "\n",
    "//60.15%\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "\n",
    "//61.46%, 60.08\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "\n",
    "//60.65%\n",
    "model.add(Dense(16, activation='relu'))\n",
    "\n",
    "//60.26%\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "\n",
    "//59%\n",
    "model.add(Dense(8, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l5GB-FkZX_Dl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H-qWPX4DX_Gb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VcYuPMyRLckB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ch7QklDnLcmY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bjUfk1_-1qSZ"
   },
   "outputs": [],
   "source": [
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=32,\n",
    "                            verbose=2)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "bow.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "kaggle_kernel",
   "language": "python",
   "name": "kaggle_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
